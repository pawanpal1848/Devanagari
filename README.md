
### Following analysis was done:
- Effect of increasing the number of hidden units in our artificial neural network.
- Effect of varying(increasing) the learning rate.
- Effect of changing the actiavation function type from Ramp to Sigmoid to Hyperbolic tangent activation function.
- Effect of changing the optimizer(We will replace the steepest gradient descent optimizer with the more sophisticated 
  ADAM optimizer)
